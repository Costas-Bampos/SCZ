{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Through a comprehensive online resource generated by the PsychENCODE Consortium (PEC) \n",
        "for the adult brain, many entities such as functional elements, quantitative-trait\n",
        "loci (QTLs), and regulatory-network linkages have been identified and embedded into a\n",
        "comprehensive deep learning model in order to predict psychiatric phenotypes from genotypic\n",
        "and transcriptomic data. The end-result is a biologically relevant Deep Boltzmann Machine\n",
        "architecture connecting genotype, functional genomics, and phenotype data, with conditional\n",
        "and lateral connections that improve trait prediction over traditional additive models.\n",
        "Our main goal is to follow a more segregating approach by implementing different\n",
        "machine learning algorithms that exploit different priors accordingly. We show that \n",
        "by knowing just the links between genotypic (TG) and transcriptomic (TF) data, we are able to \n",
        "reach a very good accuracy in classifying schizophrenic (SCZ) patients from controls.\n",
        "\n",
        "The first Pytorch Model we used for patient classification consists of a network \n",
        "of stacked Restricted Boltzmann machines (RBMs). In the first part of the code \n",
        "we add layers of RBMs sequentially and assess the accuracy of each network architecture. \n",
        "Our aim is to define the number of RBM layers we are going to use to build our final \n",
        "classifier. Firstly, we start by training RBMs and stack them sequentially to build \n",
        "denoising autoencoders (DAEs). Then we refine the RBM weights through a few consecutive \n",
        "rounds of DAE training. We then transfer the learnt weights from the DAE with the \n",
        "largest accuracy to other network architectures (mostly feedforward networks) for \n",
        "further refinement. In this initial code, we apply a MASK (representing the TF-TG \n",
        "links) to the first RBM layer in order to keep gradients steady during backpropagation. \n",
        "Except this MASK we implemented to only the first RBM, we don't enforce any further sparsity. \n",
        "\n",
        "In the second Pytorch Model we build for patient classification, we enforce sparsity \n",
        "through MASK (first layer) and through Kullbackâ€“Leibler divergence/L2-regularization \n",
        "(to all other layers), and transfer the weights to another model for further refinement \n",
        "and final classification.\n",
        "\n",
        "Below we present the second Pytorch Model applied to only one of the 10 PEC datasets.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "1WiF8HBNNINX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#__________STEP I___________ Download the input data from the following link:\n",
        "#                            https://drive.google.com/file/d/10uRj-4gd9wFaDEnEKNDRTKIn1DSTf0Hq/view?usp=sharing\n",
        "\n",
        "#__________STEP II__________ Unzip the data to google drive: \n",
        "#                            \"/content/drive/My Drive/datasets/\"\n",
        "\n",
        "#__________STEP III_________ Set Google Colab in GPU mode\n",
        "\n",
        "#__________STEP IV__________ Please run the codes below sequentially.\n",
        "#                            Even on GPU, CODE_I needs around 3.5h to complete, \n",
        "#                            but feel free to skip directly to CODE_II \n"
      ],
      "metadata": {
        "id": "EAf4v9vKYlTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lygru2dms_tz"
      },
      "source": [
        "%load_ext rpy2.ipython\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "###########################_________________________ PRE-PROCESSING _________________________###########################\n",
        "########################################################################################################################"
      ],
      "metadata": {
        "id": "WdHgGEJHfYIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________________________________________________________ Info on GPUs we are going to load our data.\n",
        "!mkdir chpnt                                                                   \n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "## memory footprint\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n"
      ],
      "metadata": {
        "id": "asDDJMPB1h0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#______INPUT FILES    :     topC.Rdata           (Transcription Factors-Genes connectivity matrix)\n",
        "#                           GRN_1_genes.csv      (Genes(ensembl_gene_id) connected to transcription factors)\n",
        "#                           Tot_TG.csv           (Total number of genes(ensembl_gene_id))\n",
        "#                           scz_data5.mat        (Gene dataset downloaded from PEC(ensembl_gene_id))\n",
        "\n",
        "#_______________________________________________________________________________ I. Converting input genes from ensembl_gene_id to hgnc_symbol\n",
        "%%R                                                                            \n",
        "\n",
        "load(file=\"/content/drive/My Drive/datasets/topC.Rdata\")\n",
        "GRN_1_TG <- read.csv(\"/content/drive/My Drive/datasets/GRN_1_genes.csv\")  \n",
        "C_TGe2 <- read.csv(\"/content/drive/My Drive/datasets/Tot_TG.csv\")\n",
        "#_______________________________________________________________________________ Ia. Convert input genes\n",
        "\n",
        "install.packages(\"R.matlab\")\n",
        "library(R.matlab)\n",
        "scz_data1 <- readMat(\"/content/drive/My Drive/datasets/scz_data5.mat\")\n",
        "scz_data1f <- data.frame(\"X.geneIds\"=matrix(unlist(scz_data1$X.geneIds), \n",
        "                                            nrow=length(scz_data1$X.geneIds), \n",
        "                                            byrow=TRUE),stringsAsFactors=FALSE)\n",
        "\n",
        "scz_data1f$X.geneIds <- as.character(scz_data1f$X.geneIds)\n",
        "scz_data1f$X.geneIds <- sub(\"[.][0-9]*\",\"\",scz_data1f$X.geneIds)\n",
        "genes1 <-  scz_data1f$X.geneIds\n",
        "\n",
        "if (!requireNamespace(\"BiocManager\", quietly = TRUE))  # Installing biomart from Bioconductor\n",
        "    install.packages(\"BiocManager\")\n",
        "BiocManager::install(\"biomaRt\", force = TRUE)\n",
        "library(biomaRt)\n",
        "\n",
        "require(\"biomaRt\")\n",
        "mart <- useMart(\"ENSEMBL_MART_ENSEMBL\")\n",
        "mart <- useDataset(\"hsapiens_gene_ensembl\", mart)\n",
        "\n",
        "ensLookup <- genes1\n",
        "\n",
        "mrg_lst2 <- getBM(\n",
        "  mart=mart,\n",
        "  attributes=c(\"ensembl_gene_id\",\"gene_biotype\",\"hgnc_symbol\"),\n",
        "  filter=\"ensembl_gene_id\",\n",
        "  values=ensLookup,\n",
        "  uniqueRows=TRUE)\n",
        "\n",
        "mrg_lst2 <- data.frame(ensLookup[match(mrg_lst2$ensembl_gene_id, ensLookup)],\n",
        "  mrg_lst2)\n",
        "\n",
        "mrg_lst2 <- subset(mrg_lst2, (!is.na(mrg_lst2['hgnc_symbol'])))\n",
        "\n",
        "colnames(mrg_lst2) <- c(\n",
        "  \"original_id\",\n",
        "  c(\"ensembl_gene_id\",\"gene_biotype\",\"hgnc_symbol\"))\n",
        "#_______________________________________________________________________________ Ib. Convert total genes\n",
        "\n",
        "scz_data1h <- data.frame(\"hgnc_symbol\"=matrix(unlist(C_TGe2$Var1), nrow=length(C_TGe2$Var1), byrow=TRUE),stringsAsFactors=FALSE)\n",
        "genes2 <-  scz_data1h$hgnc_symbol\n",
        "\n",
        "ensLookup <- genes2\n",
        "\n",
        "mrg_lst <- getBM(\n",
        "  mart=mart,\n",
        "  attributes=c(\"hgnc_symbol\",\"gene_biotype\",\"ensembl_gene_id\"),\n",
        "  filter=\"hgnc_symbol\",\n",
        "  values=ensLookup,\n",
        "  uniqueRows=TRUE)\n",
        "\n",
        "mrg_lst <- data.frame(ensLookup[match(mrg_lst$hgnc_symbol, ensLookup)],\n",
        "  mrg_lst)\n",
        "\n",
        "mrg_lst <- subset(mrg_lst, (!is.na(mrg_lst['ensembl_gene_id'])))\n",
        "\n",
        "colnames(mrg_lst) <- c(\n",
        "  \"original_id\",\n",
        "  c(\"ensembl_gene_id\",\"gene_biotype\",\"hgnc_symbol\"))\n"
      ],
      "metadata": {
        "id": "XuxBdAmhkkZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#______INPUT FILES    :     ifx4.Rdata          (indexing of Genes(hgnc_symbol) connected to Transcription Factors)\n",
        "#                           ifx6_tmp.Rdata      (indexing of Genes(hgnc_symbol) connected to Transcription Factors aligned to TopC connectivity matrix)\n",
        "#                           GNT_NAME.Rdata      (unique gene names aligned to input genes)\n",
        "\n",
        "#_______________________________________________________________________________ I. Loading input files and libraries\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n",
        "import numba as nb\n",
        "from numba.typed import List\n",
        "import numba\n",
        "!pip install -U \"ray[tune]\"\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.trial import ExportFormat\n",
        "from functools import partial\n",
        "import os\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import sparse, io \n",
        "import torchvision.datasets\n",
        "import torchvision.models\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.utils.data as data_utils\n",
        "from torchvision.datasets import MNIST\n",
        "from sklearn.datasets import make_blobs\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn.functional as F\n",
        "import rpy2.robjects as robjects\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import f_regression\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from pickle import dump\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import random as python_random\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "np.random.seed(0)\n",
        "from sklearn import datasets\n",
        "from google.colab import output\n",
        "%matplotlib inline\n",
        "\n",
        "%R load(\"/content/drive/My Drive/datasets/ifx4.Rdata\")\n",
        "%R load(\"/content/drive/My Drive/datasets/ifx6_tmp.Rdata\")\n",
        "\n",
        "#_______________________________________________________________________________ II. Keep only Genes(hgnc_symbol) connected to Transcription Factors, \n",
        "#                                                                                    remove duplicates or NA\n",
        "\n",
        "%R n13_train <- scz_data1$X.Gene2.tr[,ifx4][,-c(4070, 5038, 5353, 6869)][,ifx6_tmp][,-10041]\n",
        "%R save(n13_train, file=\"/content/drive/My Drive/datasets/n13_train_short.Rdata\")\n",
        "\n",
        "%R n13_train_trait <- scz_data1$X.Trait.tr\n",
        "%R save(n13_train_trait, file=\"/content/drive/My Drive/datasets/n13_train_trait_short.Rdata\")\n",
        "\n",
        "%R n13_test <- scz_data1$X.Gene2.te[,ifx4][,-c(4070, 5038, 5353, 6869)][,ifx6_tmp][,-10041]\n",
        "%R save(n13_test, file=\"/content/drive/My Drive/datasets/n13_test_short.Rdata\")\n",
        "\n",
        "%R n13_test_trait <- scz_data1$X.Trait.te\n",
        "%R save(n13_test_trait, file=\"/content/drive/My Drive/datasets/n13_test_trait_short.Rdata\")\n",
        "\n",
        "#_______________________________________________________________________________  III. Load files\n",
        "%R load(\"/content/drive/My Drive/datasets/n13_train_short.Rdata\")\n",
        "%R load(\"/content/drive/My Drive/datasets/n13_test_short.Rdata\")\n",
        "%R load(\"/content/drive/My Drive/datasets/n13_train_trait_short.Rdata\")\n",
        "%R load(\"/content/drive/My Drive/datasets/n13_test_trait_short.Rdata\")\n",
        "%R load(\"/content/drive/My Drive/datasets/topC.Rdata\")\n",
        "%R load(\"/content/drive/My Drive/datasets/GNT_NAME.Rdata\")\n",
        "\n",
        "scz_data2_train_tmp = robjects.r['n13_train']\n",
        "scz_data2_test_tmp = robjects.r['n13_test']\n",
        "X_train_fl_tmp = pd.DataFrame(np.array(scz_data2_train_tmp)).astype(float, 64)\n",
        "\n",
        "%R n13_train_mat <- as.matrix(n13_train)\n",
        "%R n13_test_mat <- as.matrix(n13_test)\n",
        "\n",
        "%R colnames(n13_train_mat) <- GNT_NAME$name\n",
        "%R colnames(n13_test_mat) <- GNT_NAME$name\n",
        "\n",
        "scz_data2_train = robjects.r['n13_train_mat']\n",
        "scz_data2_test = robjects.r['n13_test_mat']\n",
        "scz_data2_train_trait = robjects.r['n13_train_trait']\n",
        "scz_data2_test_trait = robjects.r['n13_test_trait']\n",
        "\n",
        "X_train_fl = pd.DataFrame(np.array(scz_data2_train)).astype(float, 64)\n",
        "y_train = pd.DataFrame(np.array(scz_data2_train_trait)[:,1]).astype(int)\n",
        "\n",
        "X_test_fl = pd.DataFrame(np.array(scz_data2_test)).astype(float, 64)\n",
        "y_test = pd.DataFrame(np.array(scz_data2_test_trait)[:,1]).astype(int)\n",
        "\n",
        "y_train.insert(0, \"Patient\", [i for i in range(len(y_train))], True)\n",
        "y_train.rename(columns = {'Patient':'Patient', \n",
        "                       0:'Disease'}, inplace = True)\n",
        "\n",
        "y_test.insert(0, \"Patient\", [i for i in range(len(y_test))], True)\n",
        "y_test.rename(columns = {'Patient':'Patient', \n",
        "                       0:'Disease'}, inplace = True)\n",
        "#_______________________________________________________________________________ IV. we select 275 random controls and 275 patients \n",
        "#                                                                                    (due to memory restrictions in Ray-tunes)\n",
        "\n",
        "n1 = 275 \n",
        "n2 = 275\n",
        "\n",
        "idx1 = X_train_fl.index.values[y_train['Disease'] == 0]\n",
        "idx2 = X_train_fl.index.values[y_train['Disease'] == 1]\n",
        "len1 = len(idx1) \n",
        "len2 = len(idx2)\n",
        "\n",
        "draw1 = np.random.permutation(len1)[:n1]\n",
        "idx1_test = idx1[draw1]\n",
        "draw2 = np.random.permutation(len2)[:n2]\n",
        "idx2_test = idx2[draw2]\n",
        "idx_test = np.hstack([idx1_test, idx2_test])\n",
        "\n",
        "idx_train = X_train_fl.index.values[idx_test]\n",
        "\n",
        "X_train_fl = X_train_fl.loc[idx_train, :]  # optional: .reset_index(drop=True)\n",
        "y_train = y_train.loc[idx_train, :]\n",
        "\n",
        "y_train.loc[y_train['Disease']==1].agg(['nunique','count','size'])  # check number of classes(equal \"0\" and \"1\")\n",
        "\n",
        "# Apply the same scaling to both datasets\n",
        "scaler = StandardScaler()\n",
        "#scaler = MinMaxScaler()\n",
        "\n",
        "X_train_scl = scaler.fit_transform(X_train_fl)\n",
        "X_test_scl = scaler.transform(X_test_fl) # note that we transform rather than fit_transform\n"
      ],
      "metadata": {
        "id": "BTHck0noRnI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#______INPUT FILES    :     genematrix_tmp.txt              (sparse connectivity Matrix)\n",
        "#                           GRN_3_mat_tmp_rownames.txt      (names of Genes connected to Transcription Factors)\n",
        "#                           GRN_3_mat_tmp_colnames.txt      (names of Transcription Factors connected to Genes)\n",
        "\n",
        "#_______________________________________________________________________________ I. PCA applied on our data\n",
        "pca = PCA()\n",
        "pca.fit_transform(X_train_scl)\n",
        "total = sum(pca.explained_variance_)\n",
        "k = 0\n",
        "current_variance = 0\n",
        "while current_variance/total < 0.90:\n",
        "    current_variance += pca.explained_variance_[k]\n",
        "    k = k + 1\n",
        "#_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Ia. Use PCA to remove less important data features\n",
        "x_new = pca.fit_transform(X_train_scl)\n",
        "pca.explained_variance_ratio_\n",
        "n_pcs= pca.components_.shape[0]\n",
        "\n",
        "x = 0\n",
        "y = []\n",
        "i1 = 0\n",
        "lim = 0.90                 #____________ let's set a limit to 90% total variance\n",
        "for i in range(pca.components_.shape[0]):\n",
        "  i1 += 1\n",
        "  x += pca.explained_variance_ratio_[i]\n",
        "  #print(x)\n",
        "  y.append(x % lim)\n",
        "  #print(y)\n",
        "  if len(y)>1:\n",
        "    if y[i-1]<y[i] and x>lim:\n",
        "      break\n",
        "x1 = 0\n",
        "for i in range(i1-1):\n",
        "  x1 += pca.explained_variance_ratio_[i]\n",
        "print(\"remove 5 bottom elements from total features that explain \" , x1, \" of variance.\", sep='')\n",
        "n_pcs_new = i1 - 1\n",
        "\n",
        "a1 = [[b[0] for b in sorted(enumerate(np.abs(pca.components_[i])),key=lambda i:i[1])] for i in range(n_pcs_new)]\n",
        "a2 = [[x for x in np.abs(pca.components_[i]) if x<np.abs(pca.components_[i])[a1[i][5]]] for i in range(n_pcs_new)]\n",
        "x = [[np.abs(pca.components_[j]).tolist().index(i) for i in a2[j]] for j in range(n_pcs_new)]\n",
        "unique = list(dict.fromkeys(np.concatenate(x)))\n",
        "torch.save(unique,'/content/drive/My Drive/datasets/unique_tmp5.pt')\n",
        "\n",
        "n_sparse = io.mmread('/content/drive/My Drive/datasets/genematrix_tmp.txt')\n",
        "\n",
        "var_names = np.genfromtxt('/content/drive/My Drive/datasets/GRN_3_mat_tmp_rownames.txt', dtype=str)\n",
        "col_names = np.genfromtxt('/content/drive/My Drive/datasets/GRN_3_mat_tmp_colnames.txt', dtype=str)\n",
        "\n",
        "gene_mat = [pd.DataFrame(n_sparse.toarray(), columns=col_names, index=var_names[:-1])]\n",
        "\n",
        "list_of_arrays1 = [np.array(df) for df in gene_mat]\n",
        "df3 = pd.DataFrame(list_of_arrays1[0])\n",
        "df3_tr = df3.drop(df3.index[unique])\n",
        "list_of_arrays1 = df3_tr.to_numpy()\n",
        "\n",
        "init_layer_lat = torch.tensor(np.stack(list_of_arrays1))\n",
        "init_layer_lat = torch.squeeze(init_layer_lat, 0)\n",
        "print(np.shape(init_layer_lat))\n",
        "\n",
        "#_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Ib. Plotting PCs\n",
        "pca = PCA(n_components=k)\n",
        "X_train_pca=pca.fit_transform(X_train_scl)\n",
        "X_test_pca=pca.transform(X_test_scl)\n",
        "var_exp = pca.explained_variance_ratio_.cumsum()\n",
        "var_exp = var_exp*100\n",
        "fig = plt.figure(1, figsize = (10, 6))\n",
        "plt.bar(range(k), var_exp);\n",
        "\n",
        "pca3 = PCA(n_components=3).fit(X_train_scl)\n",
        "X_train_reduced = pca3.transform(X_train_scl)\n",
        "\n",
        "import matplotlib.colors as mcolors\n",
        "fig = plt.figure(2, figsize=(10,6 ))\n",
        "ax = Axes3D(fig, elev=-150, azim=110,)\n",
        "ax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], X_train_reduced[:, 2], c = y_train.iloc[:,1], cmap = mcolors.ListedColormap([\"blue\", \"red\"]), linewidths=10)\n",
        "ax.set_title(\"First three PCA directions\")\n",
        "ax.set_xlabel(\"1st eigenvector\")\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.set_ylabel(\"2nd eigenvector\")\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.set_zlabel(\"3rd eigenvector\")\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "\n",
        "fig = plt.figure(3, figsize = (10, 6))\n",
        "plt.scatter(X_train_reduced[:, 0],  X_train_reduced[:, 1], c = y_train.iloc[:,1].apply(pd.to_numeric), cmap = mcolors.ListedColormap([\"blue\", \"red\"]), linewidths=10)\n",
        "plt.title(\"2D Transformation of the Above Graph to x-y plane\")\n",
        "\n",
        "fig = plt.figure(4, figsize = (10, 6))\n",
        "plt.scatter(X_train_reduced[:, 0],  X_train_reduced[:, 2], c = y_train.iloc[:,1].apply(pd.to_numeric), cmap = mcolors.ListedColormap([\"blue\", \"red\"]), linewidths=10)\n",
        "plt.title(\"2D Transformation of the Above Graph to x-z plane\")\n"
      ],
      "metadata": {
        "id": "56KfWtk1fMmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#_______________________________________________________________________________ I. Load the above cleaned data to GPU, \n",
        "#                                                                                   and pytorch DataLoaders for further use\n",
        "\n",
        "df1 = pd.DataFrame(X_train_scl)\n",
        "df1_tr = df1.drop(df1.columns[unique], axis=1)\n",
        "print(\"input of train data:\")\n",
        "print(df1_tr.shape)\n",
        "X_train_scl = df1_tr.to_numpy()\n",
        "\n",
        "df2 = pd.DataFrame(X_test_scl)\n",
        "df2_te = df2.drop(df2.columns[unique], axis=1)\n",
        "print(\"input of test data:\")\n",
        "print(df2_te.shape)\n",
        "X_test_scl = df2_te.to_numpy()\n",
        "\n",
        "inp_pad = X_train_scl.shape[1]\n",
        "torch.save(inp_pad,'/content/drive/My Drive/datasets/inp_pad_tmp5.pt')\n",
        "\n",
        "#_______________________________________________________________________________ II. load to GPU\n",
        "n14_train = torch.from_numpy(np.array(X_train_scl)).float()\n",
        "n14_test = torch.from_numpy(np.array(X_test_scl)).float()\n",
        "n14_train_trait = torch.from_numpy(np.array(y_train))\n",
        "n14_train_trait=n14_train_trait[:,1]\n",
        "n14_test_trait = torch.from_numpy(np.array(y_test))\n",
        "n14_test_trait=n14_test_trait[:,1]\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "if device =='cuda:0':\n",
        "  n14_train = n14_train.to(device)\n",
        "  n14_train_trait = n14_train_trait.to(device)\n",
        "  n14_test = n14_test.to(device)\n",
        "  n14_test_trait = n14_test_trait.to(device)\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "#_______________________________________________________________________________ III. Load to pytorch DataLoaders()\n",
        "y_val = pd.DataFrame(np.array(scz_data2_train_trait)[:,1]).astype(int)\n",
        "y_val.insert(0, \"Patient\", [i for i in range(len(y_val))], True)\n",
        "y_val.rename(columns = {'Patient':'Patient', \n",
        "                       0:'Disease'}, inplace = True)\n",
        "\n",
        "mask1 = np.ones(y_val['Disease'].shape, bool)\n",
        "mask1[~idx_train] = False\n",
        "y_val = y_val[mask1]\n",
        "y_val = y_val.dropna(axis = 0, how = 'all')\n",
        "y_val.loc[y_val['Disease']==1].agg(['nunique','count','size'])\n",
        "\n",
        "\n",
        "X_val_fl = pd.DataFrame(np.array(scz_data2_train)).astype(float, 64)\n",
        "mask = np.ones(X_val_fl.shape, bool)\n",
        "mask[~idx_train] = False\n",
        "df_mask = pd.DataFrame(data=mask)\n",
        "X_val_fl = X_val_fl[df_mask]\n",
        "X_val_fl = X_val_fl.dropna(axis = 0, how = 'all')\n",
        "X_val_scl = scaler.transform(X_val_fl)\n",
        "\n",
        "df4 = pd.DataFrame(X_val_scl)\n",
        "df4_tr = df4.drop(df4.columns[unique], axis=1)\n",
        "print(\"input of train data:\")\n",
        "print(df4_tr.shape)\n",
        "X_val_scl = df4_tr.to_numpy()\n",
        "\n",
        "\n",
        "n14_val = torch.from_numpy(np.array(X_val_scl)).float()\n",
        "n14_val_trait = torch.from_numpy(np.array(y_val))\n",
        "n14_val_trait=n14_val_trait[:,1]\n",
        "\n",
        "valid = data_utils.TensorDataset(n14_val, n14_val_trait)\n",
        "valid_loader = data_utils.DataLoader(valid, batch_size=90, shuffle=True, num_workers=2)\n",
        "train_dataset = data_utils.TensorDataset(n14_train, n14_train_trait)\n",
        "train_loader = data_utils.DataLoader(train_dataset, batch_size=550, shuffle=True, num_workers=2)\n",
        "test_dataset = data_utils.TensorDataset(n14_test, n14_test_trait)\n",
        "test_loader = data_utils.DataLoader(test_dataset, batch_size=70, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "-gnpkKbcgHnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################################################\n",
        "###########################_________________________ PIPELINE _________________________###########################\n",
        "##################################################################################################################"
      ],
      "metadata": {
        "id": "_NHmPY0Ne5aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#__________CODE I______________ \n",
        "\n",
        "#______OUTPUT FILES   :     iteration_list.txt\n",
        "\n",
        "#_______________________________________________________________________________ I. DECLARING CLASSES AND FUNCTIONS\n",
        "\n",
        "def weight_pruning(w: tf.Variable, k: float) -> tf.Variable:\n",
        "    k = tf.cast(tf.round(tf.size(w, out_type=tf.float32) * tf.constant(k)), dtype=tf.int32)\n",
        "    w_reshaped = tf.reshape(w, [-1])\n",
        "    _, indices = tf.nn.top_k(tf.negative(tf.abs(w_reshaped)), k, sorted=True, name=None)\n",
        "    mask = tf.compat.v1.scatter_nd_update(tf.Variable(tf.ones_like(w_reshaped, dtype=tf.float32), name=\"mask\", trainable=False), tf.reshape(indices, [-1, 1]), tf.zeros([k], tf.float32))\n",
        "    return tf.reshape(w_reshaped * mask, tf.shape(w))\n",
        "\n",
        "                                      #_______________________ Ia. 3-layered netork (w/o batch normalization)\n",
        "class model_tmp2(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(model_tmp2, self).__init__()\n",
        "\n",
        "      self.enc1 = nn.Linear(in_features=inp_pad, out_features=669)\n",
        "      self.enc2 = nn.Linear(in_features=669, out_features=300)\n",
        "      self.enc3 = nn.Linear(in_features=300, out_features=100)\n",
        "\n",
        "      self.dec3 = nn.Linear(in_features=100, out_features=300)\n",
        "      self.dec2 = nn.Linear(in_features=300, out_features=669)\n",
        "      self.dec1 = nn.Linear(in_features=669, out_features=inp_pad)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.enc1(x))\n",
        "      x = F.relu(self.enc2(x))\n",
        "      x = F.relu(self.enc3(x))\n",
        "\n",
        "      x = F.relu(self.dec3(x))\n",
        "      x = F.relu(self.dec2(x))\n",
        "      x = torch.sigmoid(self.dec1(x))\n",
        "      return x\n",
        "                                      #______________________ Ib. 3-layered network (with batch normalization)\n",
        "class model_tmp3(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(model_tmp3, self).__init__()\n",
        "\n",
        "      self.fc1 = nn.Linear(in_features=inp_pad, out_features=669)\n",
        "      self.fc1_bn=nn.BatchNorm1d(669)\n",
        "      self.fc2 = nn.Linear(in_features=669, out_features=300)\n",
        "      self.fc2_bn=nn.BatchNorm1d(300)\n",
        "      self.fc3 = nn.Linear(in_features=300, out_features=100)\n",
        "      self.fc3_bn=nn.BatchNorm1d(100)\n",
        "      self.fc4 = nn.Linear(in_features=100, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.fc1_bn(self.fc1(x)))\n",
        "      x = F.relu(self.fc2_bn(self.fc2(x)))\n",
        "      x = F.relu(self.fc3_bn(self.fc3(x)))\n",
        "      x = torch.sigmoid(self.fc4(x))\n",
        "      return x\n",
        "\n",
        "\n",
        "#______________________________ II/III. train_gene() performs model_2 training (autoencoder) in Pytorch from sctratch. \n",
        "                                # After enforcing sparsity through mask (first layer*) or KL_divergence/L2 (to all other layers),\n",
        "                                # we transfer the weights to model_3 for further refinement and final classification.\n",
        "                                # (*We can also choose to implement sparsity to the first layer instead of applying a mask.)\n",
        "#_______________________________________________________________________________ The function performs 80 training sessions, \n",
        "#                                                                                with 7 cross-validations each. In every cross-validation\n",
        "#                                                                                the random.seeds must be different\n",
        "\n",
        "def train_gene(config, checkpoint_dir='/content/drive/My Drive/datasets/chpnt/', data_dir=None):\n",
        "\n",
        "                                          #_____________________________________IIa. In each cross-validation the random seeds must change!\n",
        "                                                                               # The initialization of all seeds occur in main() below.\n",
        "  checkpoint_dir1 = None                  \n",
        "  checkpoint_dir = '/content/drive/My Drive/datasets/chpnt/'\n",
        "  step = 0\n",
        "  print(\"step______________________:\")\n",
        "  print(step)\n",
        "  count_dl4 = []\n",
        "  count_dl4.append(1)\n",
        "  with open('/content/drive/My Drive/datasets/count_dl5.txt', 'a') as f4:\n",
        "    print(count_dl4, file=f4)\n",
        "  if os.path.exists('/content/drive/My Drive/datasets/count_dl5.txt'):\n",
        "    count_dl4_num = open('/content/drive/My Drive/datasets/count_dl5.txt').read().count('\\n')\n",
        "    print(\"count_dl4_num________________A\")\n",
        "    print(count_dl4_num)\n",
        "  if (count_dl4_num > 7):          #__(1/2)___number of cross-validations : 7\n",
        "    print(\"______________FILE DELETED in train_gene() _____________\")\n",
        "    os.remove('/content/drive/My Drive/datasets/count_dl5.txt')\n",
        "    count_dl4 = []\n",
        "\n",
        "  count_dl4_chp = []\n",
        "  count_dl4_chp.append(1)\n",
        "  with open('/content/drive/My Drive/datasets/count_dl5_chp.txt', 'a') as f4:\n",
        "    print(count_dl4_chp, file=f4)\n",
        "  if os.path.exists('/content/drive/My Drive/datasets/count_dl5_chp.txt'):\n",
        "    count_dl4_num_chp = open('/content/drive/My Drive/datasets/count_dl5_chp.txt').read().count('\\n')\n",
        "    print(\"count_dl4_num_chp________________A\")\n",
        "    print(count_dl4_num_chp)\n",
        "\n",
        "  np.random.seed(1234+count_dl4_num)\n",
        "  python_random.seed(1234+count_dl4_num)\n",
        "  tf.random.set_seed(1234+count_dl4_num)\n",
        "  DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  if DEVICE =='cuda':\n",
        "    torch.cuda.manual_seed_all(356+count_dl4_num)\n",
        "  else:\n",
        "    torch.manual_seed(356+count_dl4_num)\n",
        "    \n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "  if device =='cuda:0':\n",
        "    torch.cuda.manual_seed_all(356+count_dl4_num)\n",
        "  else:\n",
        "    torch.manual_seed(356+count_dl4_num) \n",
        "\n",
        "################################################################################\n",
        "##########_______________________ MODEL_2 _______________________###############\n",
        "################################################################################\n",
        "\n",
        "#_______________________________________________________________________________ IIb. Load model_2 to GPU\n",
        "\n",
        "  model_2 = model_tmp2().to(device)\n",
        "\n",
        "#_______________________________________________________________________________ IIc. Optimizers  \n",
        "  if checkpoint_dir1:\n",
        "        print(\"Loading from checkpoint.\")\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model_2.load_state_dict(model_state)\n",
        "        optimizer1.load_state_dict(optimizer_state)\n",
        "        step = checkpoint[\"step\"]\n",
        "\n",
        "  if config['optimizer'] == 'optimizer_ADAM':\n",
        "\n",
        "    optimizer1 = optim.Adam(filter(lambda p: p.requires_grad, model_2.parameters()), \n",
        "                                lr=config.get(\"lr\", 0.01))\n",
        "    if \"netD_lr\" in config:\n",
        "            for param_group in optimizer1.param_groups:\n",
        "                param_group[\"lr\"] = config[\"netD_lr\"]\n",
        "\n",
        "  elif config['optimizer'] == 'optimizer_SGD':\n",
        "    optimizer1 = torch.optim.SGD([{'params':model_2.enc1.parameters()}, \n",
        "                                 {'params':model_2.enc2.parameters()},\n",
        "                                 {'params':model_2.enc3.parameters()}],\n",
        "                                  lr = 0.01, momentum = 5e-4)\n",
        "    if \"netE1_lr\" in config:\n",
        "                optimizer1.param_groups[0][\"lr\"] = config[\"netE1_lr\"]\n",
        "    if \"netE1_mom\" in config:\n",
        "                optimizer1.param_groups[0][\"momentum\"] = config[\"netE1_mom\"]\n",
        "    if \"netE2_lr\" in config:\n",
        "                optimizer1.param_groups[1][\"lr\"] = config[\"netE2_lr\"]\n",
        "    if \"netE2_mom\" in config:\n",
        "                optimizer1.param_groups[1][\"momentum\"] = config[\"netE2_mom\"]\n",
        "    if \"netE3_lr\" in config:\n",
        "                optimizer1.param_groups[2][\"lr\"] = config[\"netE3_lr\"]\n",
        "    if \"netE3_mom\" in config:\n",
        "                optimizer1.param_groups[2][\"momentum\"] = config[\"netE3_mom\"]\n",
        "\n",
        "  elif config['optimizer'] == 'optimizer_ADAgrad':                                            \n",
        "    optimizer1 = optim.Adagrad(filter(lambda p: p.requires_grad, model_2.parameters()), \n",
        "                                lr=config.get(\"lr\", 0.01))\n",
        "\n",
        "    if \"netF_lr\" in config:\n",
        "            for param_group in optimizer1.param_groups:\n",
        "                param_group[\"lr\"] = config[\"netF_lr\"]\n",
        "\n",
        "  elif config['optimizer'] == 'optimizer_RMS':\n",
        "    optimizer1 = optim.RMSprop(filter(lambda p: p.requires_grad, model_2.parameters()), \n",
        "                                lr=config.get(\"lr\", 0.01))\n",
        "        \n",
        "    if \"netG_lr\" in config:\n",
        "            for param_group in optimizer1.param_groups:\n",
        "                param_group[\"lr\"] = config[\"netG_lr\"]\n",
        "                                    \n",
        "                                              #_________________________________ IId. model_2 training\n",
        "\n",
        "  scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1, step_size=config[\"step_size1\"], gamma=config[\"gamma1\"])\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "  #criterion = nn.MSELoss()\n",
        "  test_loss = [] \n",
        "  train_loss = []\n",
        "  tmp_acc = []\n",
        "  rh0 = 0.05\n",
        "  BATCH_SIZE_tr = 550\n",
        "  BATCH_SIZE_te = 70\n",
        "  BETA = 0.001\n",
        "  nb1 = config[\"nb1\"]\n",
        "  MNIST_NUM_PIXELS = inp_pad\n",
        "\n",
        "  summ_spars = []\n",
        "  count = 3\n",
        "  prm4 = []\n",
        "  model_children = list(model_2.children())\n",
        "\n",
        "  MASK.requires_grad = False   # MASKed neurons cannot be trained\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model_2.enc1.weight.register_hook(lambda grad: grad.mul_(MASK.to(device)))\n",
        "    model_2.dec1.weight.register_hook(lambda grad: grad.mul_(MASK.t().to(device)))\n",
        "  else:\n",
        "    model_2.enc1.weight.register_hook(lambda grad: grad.mul_(MASK))\n",
        "    model_2.dec1.weight.register_hook(lambda grad: grad.mul_(MASK.t()))\n",
        "  \n",
        "  train_losses, dev_losses, train_acc, train_pcc, dev_acc, dev_pcc, dev_fcc, dev_rcc = [], [], [], [], [], [], [], []\n",
        "  for epoch in range(nb1):\n",
        "      running_loss = 0.0\n",
        "      features = np.zeros((BATCH_SIZE_tr, MNIST_NUM_PIXELS))\n",
        "      labels = np.zeros(BATCH_SIZE_tr)\n",
        "\n",
        "      model_2.train()\n",
        "      l2_reg = 0\n",
        "      features_train = n14_train\n",
        "      optimizer1.zero_grad()\n",
        "      sparsity = 0\n",
        "      sparsity1 = 0\n",
        "      sparsity2 = 0\n",
        "      sparsity3 = 0\n",
        "      sparsity4 = 0\n",
        "      values = features_train\n",
        "      features = model_2(features_train)\n",
        "      labels = n14_train_trait\n",
        "      mse_loss = criterion(features,features_train)\n",
        "\n",
        "                                                  #_____________________________ IIe. We calculate sparsity only to the encoder part(3/6 layers)\n",
        "      for i in range(len(model_children)):\n",
        "        values = model_children[i](values)\n",
        "        if (i == 0):\n",
        "          values1 = torch.mean(torch.sigmoid(values), 1)\n",
        "          rho1 = torch.tensor([config[\"RHO1\"]] * len(values1)).to(device)\n",
        "          sparsity1 += torch.sum(rho1 * torch.log(rho1/values1) + (1 - rho1) * torch.log((1 - rho1)/(1.0001 - values1)))\n",
        "        if (i == 1):           \n",
        "          values1 = torch.mean(torch.sigmoid(values), 1)\n",
        "          rho2 = torch.tensor([config[\"RHO2\"]] * len(values1)).to(device)\n",
        "          sparsity2 += torch.sum(rho2 * torch.log(rho2/values1) + (1 - rho2) * torch.log((1 - rho2)/(1.0001 - values1)))\n",
        "        if (i == 2):\n",
        "          values1 = torch.mean(torch.sigmoid(values), 1)\n",
        "          rho3 = torch.tensor([config[\"RHO3\"]] * len(values1)).to(device)\n",
        "          sparsity3 += torch.sum(rho3 * torch.log(rho3/values1) + (1 - rho3) * torch.log((1 - rho3)/(1.0001 - values1)))\n",
        "\n",
        "      for name, param in model_2.named_parameters():\n",
        "        if 'weight' in name:\n",
        "          l2_reg += param.pow(2).sum() / 2\n",
        "\n",
        "      sparsity1 = sparsity1.clone().detach().requires_grad_(True)\n",
        "      sparsity2 = sparsity2.clone().detach().requires_grad_(True)\n",
        "      sparsity3 = sparsity3.clone().detach().requires_grad_(True)\n",
        "      sparsity = sparsity1 + sparsity2 + sparsity3\n",
        "\n",
        "      l2_reg = l2_reg.clone().detach().requires_grad_(True)\n",
        "\n",
        "      if config['reg'] == 'sparse_norm':\n",
        "        #loss = mse_loss + config['BETA'] * sparsity   # KL Divergence (for total sparsity in the whole network)\n",
        "        loss = (mse_loss + config['BETA1'] * sparsity1\n",
        "                                 + config['BETA2'] * sparsity2 \n",
        "                                       + config['BETA3'] * sparsity3) # KL Div (for sparsities in individual layers)\n",
        "      if config['reg'] == 'L2_norm':\n",
        "        loss = mse_loss + config['weight_decay'] * l2_reg   # L2 regularization\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer1.step()\n",
        "      scheduler1.step()\n",
        "      running_loss += loss.item()\n",
        "      epoch_loss = running_loss\n",
        "      train_losses.append(epoch_loss)\n",
        "\n",
        "      mse_loss = 0\n",
        "      pp = 0\n",
        "      running_loss = 0.0\n",
        "      epoch_loss = 0.0\n",
        "      loss = 0.0\n",
        "      acc = 0.0\n",
        "                                            # __________________________________ IIf. Model_2 testing \n",
        "      for param in model_2.parameters():\n",
        "            param.requires_grad = False\n",
        "      with torch.set_grad_enabled(False):\n",
        "            model_2.eval()\n",
        "            features_test = np.zeros((BATCH_SIZE_te, MNIST_NUM_PIXELS))\n",
        "            labels_test = np.zeros(BATCH_SIZE_te)\n",
        "            features_test_tmp = n14_test\n",
        "            features_test = model_2(features_test_tmp)\n",
        "            labels_test = n14_test_trait\n",
        "            mse_loss = criterion(features_test, features_test_tmp)\n",
        "            loss = mse_loss\n",
        "            running_loss = loss.item()\n",
        "            epoch_loss = running_loss\n",
        "            dev_losses.append(epoch_loss)\n",
        "                                                    #___________________________ IIg. All layers can be freely trained again\n",
        "      for name, param in model_2.named_parameters():\n",
        "            if 'enc1.weight' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc1.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc2.weight' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc2.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc3.weight' in name:\n",
        "              param.requires_grad = True            \n",
        "            if 'enc3.bias' in name:\n",
        "              param.requires_grad = True\n",
        "\n",
        "            if 'dec1.weight' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'dec1.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'dec2.weight' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'dec2.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'dec3.weight' in name:\n",
        "              param.requires_grad = True            \n",
        "            if 'dec3.bias' in name:\n",
        "              param.requires_grad = True                                                         \n",
        "\n",
        "  if checkpoint_dir1:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model_2.load_state_dict(model_state)\n",
        "        optimizer1.load_state_dict(optimizer_state)\n",
        "\n",
        "  #_____________________________________________________________________________ IIh. Pruning techniques\n",
        "\n",
        "                                                #________ IIh1. pruning layer-1 through MASK\n",
        "  MASK.requires_grad = False\n",
        "  \n",
        "  for name, param in model_2.named_parameters():\n",
        "    param.requires_grad = False\n",
        "    if 'enc1.weight' in name:\n",
        "      if torch.cuda.is_available():\n",
        "        param1 = tf.convert_to_tensor(param.detach().cpu().numpy()) * MASK\n",
        "      else:\n",
        "        param1 = tf.convert_to_tensor(param) * MASK     \n",
        "      parm4 = torch.from_numpy(param1.numpy())\n",
        "      param.copy_(parm4)\n",
        "\n",
        "  for name, param in model_2.named_parameters():\n",
        "    param.requires_grad = False\n",
        "    if 'dec1.weight' in name:\n",
        "      if torch.cuda.is_available():\n",
        "        param1 = tf.convert_to_tensor(param.detach().cpu().numpy()) * MASK.t()\n",
        "      else:\n",
        "        param1 = tf.convert_to_tensor(param) * MASK.t()      \n",
        "      parm4 = torch.from_numpy(param1.numpy())\n",
        "      param.copy_(parm4)\n",
        "\n",
        "                                                #________ IIh2. pruning the rest of layers through KL_div\n",
        "  for name, param in model_2.named_parameters():\n",
        "    param.requires_grad = False\n",
        "    if 'enc2.weight' in name:\n",
        "      if torch.cuda.is_available():\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param.detach().cpu().numpy()), rho2[0].cpu())\n",
        "      else:\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param), rho2[0])\n",
        "      parm4 = torch.from_numpy(param1.numpy())\n",
        "      param.copy_(parm4)\n",
        "    if 'enc3.weight' in name:\n",
        "      if torch.cuda.is_available():\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param.detach().cpu().numpy()), rho3[0].cpu())\n",
        "      else:\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param),  rho3[0])          \n",
        "      parm4 = torch.from_numpy(param1.numpy())\n",
        "      param.copy_(parm4)     \n",
        "    if 'dec2.weight' in name:\n",
        "      if torch.cuda.is_available():\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param.detach().cpu().numpy()), rho2[0].cpu())\n",
        "      else:\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param), rho2[0])           \n",
        "      parm4 = torch.from_numpy(param1.numpy())\n",
        "      param.copy_(parm4)      \n",
        "    if 'dec3.weight' in name:\n",
        "      if torch.cuda.is_available():\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param.detach().cpu().numpy()), rho3[0].cpu())\n",
        "      else:\n",
        "        param1 = weight_pruning(tf.convert_to_tensor(param), rho3[0])           \n",
        "      parm4 = torch.from_numpy(param1.numpy())\n",
        "      param.copy_(parm4)      \n",
        "  model_children = list(model_2.children())\n",
        "\n",
        "  #_____________________________________________________________________________ IIi. Estimate sparsity per layer \n",
        "  with open('/content/Sparsity.txt', 'a') as f3:\n",
        "    print(\"Sparsity in enc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model_2.enc1.weight == 0))\n",
        "        / float(model_2.enc1.weight.nelement())), file=f3)\n",
        "    print(\"Sparsity in enc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model_2.enc2.weight == 0))\n",
        "        / float(model_2.enc2.weight.nelement())), file=f3)\n",
        "    print(\"Sparsity in enc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model_2.enc3.weight == 0))\n",
        "        / float(model_2.enc3.weight.nelement())), file=f3)\n",
        "    print(\"Sparsity in dec1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model_2.dec1.weight == 0))\n",
        "        / float(model_2.dec1.weight.nelement())), file=f3)\n",
        "    print(\"Sparsity in dec2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model_2.dec2.weight == 0))\n",
        "        / float(model_2.dec2.weight.nelement())), file=f3)\n",
        "    print(\"Sparsity in dec3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model_2.dec3.weight == 0))\n",
        "        / float(model_2.dec3.weight.nelement())), file=f3)\n",
        "    print(\"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model_2.enc1.weight == 0)\n",
        "            + torch.sum(model_2.enc2.weight == 0)\n",
        "            + torch.sum(model_2.enc3.weight == 0)\n",
        "            + torch.sum(model_2.dec1.weight == 0)\n",
        "            + torch.sum(model_2.dec2.weight == 0)\n",
        "            + torch.sum(model_2.dec3.weight == 0))\n",
        "        / float(\n",
        "            model_2.enc1.weight.nelement()\n",
        "            + model_2.enc2.weight.nelement()\n",
        "            + model_2.enc3.weight.nelement()\n",
        "            + model_2.dec1.weight.nelement()\n",
        "            + model_2.dec2.weight.nelement()\n",
        "            + model_2.dec3.weight.nelement())), file=f3)\n",
        "    print(\"___________________________________________\\n\", file=f3) \n",
        "\n",
        "\n",
        "  ##############################################################################\n",
        "  ##########_______________________ MODEL_3 _______________________#############\n",
        "  ##############################################################################\n",
        "\n",
        "  #_____________________________________________________________________________ IIIa. Load model_3 to GPU\n",
        "  model_3 = model_tmp3().to(device)\n",
        "\n",
        "#  GPUs = GPU.getGPUs()   #_____________________________________________________ IIIb. Uncomment to watch the available RAM during training\n",
        "#  gpu = GPUs[0]\n",
        "#  def printm():\n",
        "#    process = psutil.Process(os.getpid())\n",
        "#    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "#    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "#  printm()\n",
        "\n",
        "  #_____________________________________________________________________________ IIIc. weight tranfer from model_2 to model_3, and weight freezing\n",
        "  mdl_2_name = []\n",
        "  mdl_2_prms = []\n",
        "\n",
        "  for name,param in model_2.named_parameters():\n",
        "    mdl_2_name.append(name)\n",
        "    mdl_2_prms.append(param)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for name, param in model_3.named_parameters():\n",
        "      if 'fc1.weight' in name:\n",
        "        param.copy_(mdl_2_prms[0].clone().detach().requires_grad_(False))\n",
        "      if 'fc1.bias' in name:\n",
        "        param.copy_(mdl_2_prms[1].clone().detach().requires_grad_(False))   \n",
        "      if 'fc2.weight' in name:\n",
        "        param.copy_(mdl_2_prms[2].clone().detach().requires_grad_(False))\n",
        "      if 'fc2.bias' in name:\n",
        "        param.copy_(mdl_2_prms[3].clone().detach().requires_grad_(False))\n",
        "      if 'fc3.weight' in name:\n",
        "        param.copy_(mdl_2_prms[4].clone().detach().requires_grad_(False))\n",
        "      if 'fc3.bias' in name:\n",
        "        param.copy_(mdl_2_prms[5].clone().detach().requires_grad_(False))\n",
        "      if 'fc4.weight' in name:\n",
        "        param.requires_grad = True\n",
        "      if 'fc4.bias' in name:\n",
        "        param.requires_grad = True   \n",
        "\n",
        "  model_children = list(model_3.children())\n",
        "\n",
        "  model_3_fc1_weight_mask = MASK\n",
        "  model_3_fc1_bias_mask = [1 if val != 0 else val for val in model_3.fc1.bias]\n",
        "  model_3_fc2_weight_mask = [[1 if val != 0 else val for val in subl] for subl in model_3.fc2.weight]\n",
        "  model_3_fc2_bias_mask = [1 if val != 0 else val for val in model_3.fc2.bias]\n",
        "  model_3_fc3_weight_mask = [[1 if val != 0 else val for val in subl] for subl in model_3.fc3.weight]\n",
        "  model_3_fc3_bias_mask = [1 if val != 0 else val for val in model_3.fc3.bias]\n",
        "\n",
        "                              #_________________________________________________ IIId. Update only sub-elements of weights based on masks\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model_3.fc1.weight.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc1_weight_mask).to(device)))\n",
        "    model_3.fc1.bias.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc1_bias_mask).to(device)))\n",
        "    model_3.fc2.weight.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc2_weight_mask).to(device)))\n",
        "    model_3.fc2.bias.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc2_bias_mask).to(device)))\n",
        "    model_3.fc3.weight.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc3_weight_mask).to(device)))\n",
        "    model_3.fc3.bias.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc3_bias_mask).to(device)))\n",
        "  else:\n",
        "    model_3.fc1.weight.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc1_weight_mask)))\n",
        "    model_3.fc1.bias.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc1_bias_mask)))\n",
        "    model_3.fc2.weight.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc2_weight_mask)))\n",
        "    model_3.fc2.bias.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc2_bias_mask))) \n",
        "    model_3.fc3.weight.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc3_weight_mask)))\n",
        "    model_3.fc3.bias.register_hook(lambda grad: grad.mul_(torch.Tensor(model_3_fc3_bias_mask)))\n",
        "\n",
        "                               #________________________________________________ IIIe. Model_3 training\n",
        "\n",
        "  #criterion = nn.BCELoss()\n",
        "  criterion = nn.MSELoss()\n",
        "  nb2 = config[\"nb2\"]                                                            \n",
        "  optimizer2 = optim.Adam(filter(lambda p: p.requires_grad, model_3.parameters()), \n",
        "                       lr=config[\"learning_rate2\"])\n",
        "  scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=config[\"step_size2\"], gamma=config[\"gamma2\"])\n",
        "    \n",
        "  train_losses, dev_losses, train_acc, train_pcc, train_fcc, train_rcc, dev_acc, dev_pcc, dev_fcc, dev_rcc = [], [], [], [], [], [], [], [], [], []\n",
        "  for epoch in range(nb2):                  \n",
        "      running_loss_tr = 0.0\n",
        "      running_acc = 0.0\n",
        "      running_pcc = 0.0 \n",
        "      running_fcc = 0.0 \n",
        "      running_rcc = 0.0                \n",
        "      pp = 0\n",
        "\n",
        "      for param in model_3.parameters():\n",
        "            param.requires_grad = True\n",
        "      with torch.set_grad_enabled(True):\n",
        "            model_3.train()\n",
        "            features = np.zeros((BATCH_SIZE_tr, MNIST_NUM_PIXELS))\n",
        "            labels = np.zeros(BATCH_SIZE_tr)\n",
        "            features_train = n14_train\n",
        "            optimizer2.zero_grad()\n",
        "            sparsity = 0\n",
        "            values = features_train\n",
        "            features = model_3(features_train)\n",
        "            labels = n14_train_trait\n",
        "            if torch.cuda.is_available():\n",
        "              mse_loss = criterion(features.squeeze(), labels.float())\n",
        "            loss = mse_loss #+ BETA * sparsity          # KL Div\n",
        "            #loss = mse_loss + weight_decay * l2_reg    # L2 regularization\n",
        "            loss.backward()\n",
        "            optimizer2.step()\n",
        "            scheduler2.step()\n",
        "            running_loss_tr = loss.item()\n",
        "            top_class = (features>0.5).float()\n",
        "            if torch.cuda.is_available():        \n",
        "              running_acc = accuracy_score(labels.cpu(), top_class.detach().cpu().numpy())\n",
        "              running_pcc = precision_score(labels.cpu(), top_class.detach().cpu().numpy())\n",
        "\n",
        "              #______________________PRECISION train_________________________\n",
        "              with open('/content/PRECISION_train.txt', 'a') as f5:\n",
        "                print(count_dl4_num_chp, file = f5)\n",
        "                print(top_class.detach().cpu().numpy(), file=f5)              \n",
        "              #______________________________________________________________\n",
        "              running_fcc = f1_score(labels.cpu(), top_class.detach().cpu().numpy())\n",
        "              running_rcc = recall_score(labels.cpu(), top_class.detach().cpu().numpy())\n",
        "\n",
        "            epoch_loss = running_loss_tr\n",
        "            train_losses.append(epoch_loss)\n",
        "            train_acc.append(running_acc)\n",
        "            train_pcc.append(running_pcc)\n",
        "            train_fcc.append(running_fcc)\n",
        "            train_rcc.append(running_rcc)\n",
        "\n",
        "                               #________________________________________________ IIIf. Model_3 testing\n",
        "      mse_loss = 0\n",
        "      pp = 0\n",
        "      running_loss_te = 0.0\n",
        "      epoch_loss = 0.0\n",
        "      loss = 0.0\n",
        "      acc = 0.0\n",
        "      pcc = 0.0\n",
        "      fcc = 0.0\n",
        "      rcc = 0.0            \n",
        "      for param in model_3.parameters():\n",
        "            param.requires_grad = False\n",
        "      with torch.set_grad_enabled(False):\n",
        "            model_3.eval()\n",
        "            features_test = np.zeros((BATCH_SIZE_te, MNIST_NUM_PIXELS))\n",
        "            labels_test = np.zeros(BATCH_SIZE_te)\n",
        "            features_test_tmp = n14_test\n",
        "            features_test = model_3(features_test_tmp)\n",
        "            labels_test = n14_test_trait\n",
        "            if torch.cuda.is_available():\n",
        "              mse_loss = criterion(features_test.squeeze(), labels_test.float())\n",
        "            loss = mse_loss\n",
        "            running_loss_te = loss.item()\n",
        "            top_class_dev = (features_test>0.5).float()\n",
        "            if torch.cuda.is_available():        \n",
        "              acc = accuracy_score(labels_test.cpu(), top_class_dev.detach().cpu().numpy())\n",
        "              pcc = precision_score(labels_test.cpu(), top_class_dev.detach().cpu().numpy())\n",
        "\n",
        "              #______________________PRECISION test_________________________\n",
        "              with open('/content/PRECISION test.txt', 'a') as f6:\n",
        "                print(count_dl4_num_chp, file = f6)\n",
        "                print(top_class_dev.detach().cpu().numpy(), file=f6)              \n",
        "              #_____________________________________________________________\n",
        "              fcc = f1_score(labels_test.cpu(), top_class_dev.detach().cpu().numpy())\n",
        "              rcc = recall_score(labels_test.cpu(), top_class_dev.detach().cpu().numpy())\n",
        "\n",
        "            epoch_loss = running_loss_te\n",
        "            dev_losses.append(epoch_loss)\n",
        "            dev_acc.append(acc)\n",
        "            dev_pcc.append(pcc)\n",
        "            dev_fcc.append(fcc)\n",
        "            dev_rcc.append(rcc)\n",
        "                                       #________________________________________ IIIg. Unfreezing weights\n",
        "      for name, param in model_2.named_parameters():\n",
        "            if 'enc1.weight' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc1.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc2.weight' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc2.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc3.weight' in name:\n",
        "              param.requires_grad = True            \n",
        "            if 'enc3.bias' in name:\n",
        "              param.requires_grad = True\n",
        "            if 'enc4.weight' in name:\n",
        "              param.requires_grad = True            \n",
        "            if 'enc4.bias' in name:\n",
        "              param.requires_grad = True\n",
        "\n",
        "      if count_dl4_num_chp % 1 == 0:     # frequency of saving pth files!!!!!!!\n",
        "            chp_dir = \"chp_\"+str(count_dl4_num_chp) + \".pth\"\n",
        "            with tune.checkpoint_dir(count_dl4_num_chp) as checkpoint_dir:\n",
        "                path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "                torch.save({\"step\": count_dl4_num_chp, \"model_state_dict\": model_2.state_dict(),\n",
        "                            \"optim\": optimizer1.state_dict(), \"accuracy\": acc, \"precision\": pcc, \"f1\": fcc, \"recall\": rcc}, \"/content/chpnt\"+\"/\"+chp_dir)\n",
        "      step += 1\n",
        "      tune.report(loss = (running_loss_te), accuracy = acc, precision = pcc, f1 = fcc, recall = rcc)\n",
        "\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________ IV. Main() \n",
        "\n",
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "\n",
        "                                                #_______________________________ IVa. Search space\n",
        "    data_dir = os.path.abspath(\"checkpoint\")\n",
        "    #load_data(data_dir)\n",
        "    config = {\n",
        "        \"nb1\": tune.choice([200, 400]), \n",
        "        \"nb2\": tune.choice([200, 400]),        \n",
        "        \"optimizer\" : tune.choice([\"optimizer_ADAM\",\"optimizer_SGD\",\"optimizer_ADAgrad\",\"optimizer_RMS\"]),\n",
        "        \"reg\" : tune.choice([\"sparse_norm\",\"L2_norm\"]),\n",
        "        \"BETA\" : tune.loguniform(0.0001, 0.01),\n",
        "        \"BETA1\" : tune.loguniform(0.0001, 0.01),\n",
        "        \"BETA2\" : tune.loguniform(0.0001, 0.01),\n",
        "        \"BETA3\" : tune.loguniform(0.0001, 0.01),\n",
        "        \"weight_decay\" : tune.loguniform(0.0001, 0.01),\n",
        "        \"RHO1\":tune.loguniform(0.05, 0.4),\n",
        "        \"RHO2\":tune.loguniform(0.05, 0.4),\n",
        "        \"RHO3\":tune.loguniform(0.05, 0.4),\n",
        "        \"RHO4\":tune.loguniform(0.05, 0.4),\n",
        "        \"netD_lr\": tune.loguniform(0.000001,0.01),\n",
        "        \"netE1_lr\": tune.loguniform(0.000001,0.01),\n",
        "        \"netE2_lr\": tune.loguniform(0.000001,0.01),\n",
        "        \"netE3_lr\": tune.loguniform(0.000001,0.01),\n",
        "        \"netE1_mom\": tune.loguniform(0.1,0.99),\n",
        "        \"netE2_mom\": tune.loguniform(0.1,0.99),\n",
        "        \"netE3_mom\": tune.loguniform(0.1,0.99),\n",
        "        \"netF_lr\": tune.loguniform(0.000001,0.01),\n",
        "        \"netG_lr\": tune.loguniform(0.000001,0.01),\n",
        "        \"learning_rate2\" : tune.loguniform(0.00001, 0.01),      \n",
        "        \"gamma2\" :  tune.choice([0.01, 0.1, 1]),\n",
        "        \"step_size2\" : tune.choice([50, 100, 200]),\n",
        "        \"gamma1\" :  tune.choice([0.01, 0.1, 1]),\n",
        "        \"step_size1\" : tune.choice([50, 100, 200]),        \n",
        "        \"repeat\": tune.grid_search(list(range(7)))  # (2/2) number of cross-validations : 7\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\", \"precision\", \"f1\", \"recall\"],\n",
        "        max_progress_rows=80, max_error_rows=20, max_report_frequency=240)\n",
        "    searcher = tune.search.basic_variant.BasicVariantGenerator(\n",
        "    constant_grid_search=True)\n",
        "\n",
        "    result = tune.run(\n",
        "        partial(train_gene, data_dir=data_dir),\n",
        "        name = \"train_gene\",\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        search_alg=searcher,\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        verbose = 1,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "\n",
        "                                                            #___________________ IVb. Print results to file\n",
        "    with open('/content/report_best_config_file.txt', 'a') as f2:\n",
        "      print(\"Best trial config: {}\".format(best_trial.config), file=f2)\n",
        "      print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]), file=f2)\n",
        "      print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]), file=f2)\n",
        "      print(\"Best trial final validation precision: {}\".format(\n",
        "        best_trial.last_result[\"precision\"]), file=f2)\n",
        "      print(\"Best trial final validation f1: {}\".format(\n",
        "        best_trial.last_result[\"f1\"]), file=f2)\n",
        "      print(\"Best trial final validation recall: {}\".format(\n",
        "        best_trial.last_result[\"recall\"]), file=f2)\n",
        "      print(\"______________________________________________\\n\", file=f2)\n",
        "          \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    if os.path.exists('/content/drive/My Drive/datasets/count_dl5_chp.txt'):\n",
        "        os.remove('/content/drive/My Drive/datasets/count_dl5_chp.txt')\n",
        "\n",
        "    if os.path.exists('/content/drive/My Drive/datasets/count_dl5.txt'):\n",
        "      count_dl9 = open('/content/drive/My Drive/datasets/count_dl5.txt').read().count('\\n')\n",
        "      count_dl9=count_dl9+5\n",
        "      print(\"______________CAUTION: FILE ALREADY EXISTS_____________\")\n",
        "      print(count_dl9)\n",
        "      if (count_dl9 > 3):\n",
        "        print(\"______________FILE DELETED in main()_____________\")\n",
        "        os.remove('/content/drive/My Drive/datasets/count_dl5.txt')\n",
        "        count_dl4 = []\n",
        "                                   #____________________________________________ IVc. Initialize random seeds for numpy, tensorflow and pytorch\n",
        "    np.random.seed(1234)\n",
        "    python_random.seed(1234)\n",
        "    tf.random.set_seed(1234)\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    if DEVICE =='cuda':\n",
        "      torch.cuda.manual_seed_all(356)\n",
        "    else:\n",
        "      torch.manual_seed(356)\n",
        "    \n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "      device = \"cuda:0\"\n",
        "    if device =='cuda:0':\n",
        "      torch.cuda.manual_seed_all(356)\n",
        "    else:\n",
        "      torch.manual_seed(356)    \n",
        "                                   #____________________________________________ IVd. Initialize MASK\n",
        "\n",
        "    #a = torch.empty(669, inp_pad).uniform_(0, 1)   # for a random mask uncomment these 2 lines\n",
        "    #MASK = torch.bernoulli(a)\n",
        "    MASK = init_layer_lat.t().float()   # MASK based on specific Transcription factor-Gene connections\n",
        "    MASK.requires_grad = False\n",
        "\n",
        "                                   #____________________________________________ IVe. You can change the number of training iterations,\n",
        "#                                                                                     epochs and available GPUs here:  \n",
        "    main(num_samples=80, max_num_epochs=300, gpus_per_trial=1)\n"
      ],
      "metadata": {
        "id": "hOhFWOQSTofH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "##########################_________________________ POST-PROCESSING _________________________###########################\n",
        "########################################################################################################################"
      ],
      "metadata": {
        "id": "nbnhC9o4PzC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#__________CODE II.1______________ Run this module to analyse the output from train_gene().\n",
        "\n",
        "#______INPUT FILES   :     iteration_list.txt\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/datasets/iteration_list.txt\", sep=\"|\", usecols = [32,33,35,36,37], low_memory = True)  \n",
        "#print(df[0:])\n",
        "df1 = df.groupby(np.arange(len(df))//7).mean()\n",
        "df2 = df.groupby(np.arange(len(df))//7).std()\n",
        "\n",
        "#_____________________________________________\n",
        "print(\"All the metrics of the iteration with the maximum mean accuracy: \")\n",
        "#print(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()])                     # Max mean accuracy\n",
        "print(df1.iloc[df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0],:]) \n",
        "#print(df1.iloc[df2.loc[df2.iloc[:,1] == df2.iloc[:,1].min()].index[0],:])\n",
        "print(\"____________________________________________\")\n",
        "print(\"All the metrics of the iteration with the minimum mean loss: \")\n",
        "#print(df1.loc[df1.iloc[:,0] == df1.iloc[:,0].min()])                     # Min mean loss\n",
        "print(df1.iloc[df1.loc[df1.iloc[:,0] == df1.iloc[:,0].min()].index[0],:])\n",
        "#print(df1.iloc[df2.loc[df2.iloc[:,0] == df2.iloc[:,0].min()].index[0],:])\n",
        "\n",
        "print(\"#_________________________#_____________________________#\")\n",
        "print(\"#_________________________#_____________________________#\")\n",
        "\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/datasets/iteration_list.txt\", sep=\"|\")    #__ Best Parameters!!!\n",
        "df3 = df3[df3.index % 7 == 0]\n",
        "\n",
        "print(\"The parameters of the iteration with the maximum accuracies: \")\n",
        "print(df3.iloc[df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0],:])\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "print(\"The parameters of the iteration with the minimum loss: \")\n",
        "print(df3.iloc[df1.loc[df1.iloc[:,0] == df1.iloc[:,0].min()].index[0],:])\n",
        "\n",
        "print(\"_________________________________________________________________________\")\n",
        "\n",
        "print(\"The iteration with the maximum accuracies: \")\n",
        "dg1 = []\n",
        "for i in range(0,7):\n",
        "  print(df.iloc[df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0]*7+i,1])\n",
        "  dg1.append(df.iloc[df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0]*7+i,1])\n",
        "\n",
        "print(\"_______________________\")\n",
        "\n",
        "print(\"The iteration with the minimum loss: \")\n",
        "dg2 = []\n",
        "for i in range(0,7):\n",
        "  print(df.iloc[df1.loc[df1.iloc[:,0] == df1.iloc[:,0].min()].index[0]*7+i,0])\n",
        "  dg2.append(df.iloc[df1.loc[df1.iloc[:,0] == df1.iloc[:,0].min()].index[0]*7+i,0])\n",
        "\n",
        "minpos = dg2.index(min(dg2)) + df1.loc[df1.iloc[:,0] == df1.iloc[:,0].min()].index[0]*7 + 1\n",
        "maxpos = dg1.index(max(dg1)) + df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0]*7 + 1\n",
        "\n",
        "print(\"____________________________________\")\n",
        "print('Download chp_',minpos,'.pth')           #this is the weight matrix of the model with the minimum loss\n",
        "print('Download chp_',maxpos,'.pth')           #this is the weight matrix of the model with the maximum accuracy\n",
        "print(\"____________________________________\")\n"
      ],
      "metadata": {
        "id": "ClH7oFh7GvjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#__________CODE II.2______________  Run this module to plot the metrics from the best of the 80 training iterations, \n",
        "#                                   over the course of the 7 cross-validations.\n",
        "\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import splrep, splev\n",
        "\n",
        "rbm4 = []\n",
        "rbm8 = []\n",
        "rbm12 = []\n",
        "rbm16 = []\n",
        "rbm20 = []\n",
        "rbm24 = []\n",
        "rbm28 = []\n",
        "\n",
        "dae4 = []\n",
        "dae8 = []\n",
        "dae12 = []\n",
        "dae16 = []\n",
        "dae20 = []\n",
        "dae24 = []\n",
        "dae28 = []\n",
        "\n",
        "gbm4 = []\n",
        "gbm8 = []\n",
        "gbm12 = []\n",
        "gbm16 = []\n",
        "gbm20 = []\n",
        "gbm24 = []\n",
        "gbm28 = []\n",
        "\n",
        "kae4 = []\n",
        "kae8 = []\n",
        "kae12 = []\n",
        "kae16 = []\n",
        "kae20 = []\n",
        "kae24 = []\n",
        "kae28 = []\n",
        "\n",
        "pbm4 = []\n",
        "pbm8 = []\n",
        "pbm12 = []\n",
        "pbm16 = []\n",
        "pbm20 = []\n",
        "pbm24 = []\n",
        "pbm28 = []\n",
        "\n",
        "rbm4.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+0,0])\n",
        "rbm8.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+1,0])\n",
        "rbm12.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+2,0])\n",
        "rbm16.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+3,0])\n",
        "rbm20.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+4,0])\n",
        "rbm24.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+5,0])\n",
        "rbm28.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+6,0])\n",
        "\n",
        "dae4.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+0,1])\n",
        "dae8.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+1,1])\n",
        "dae12.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+2,1])\n",
        "dae16.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+3,1])\n",
        "dae20.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+4,1])\n",
        "dae24.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+5,1])\n",
        "dae28.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+6,1])\n",
        "\n",
        "gbm4.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+0,2])\n",
        "gbm8.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+1,2])\n",
        "gbm12.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+2,2])\n",
        "gbm16.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+3,2])\n",
        "gbm20.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+4,2])\n",
        "gbm24.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+5,2])\n",
        "gbm28.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+6,2])\n",
        "\n",
        "kae4.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+0,3])\n",
        "kae8.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+1,3])\n",
        "kae12.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+2,3])\n",
        "kae16.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+3,3])\n",
        "kae20.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+4,3])\n",
        "kae24.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+5,3])\n",
        "kae28.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+6,3])\n",
        "\n",
        "pbm4.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+0,4])\n",
        "pbm8.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+1,4])\n",
        "pbm12.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+2,4])\n",
        "pbm16.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+3,4])\n",
        "pbm20.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+4,4])\n",
        "pbm24.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+5,4])\n",
        "pbm28.append(df.iloc[(df1.loc[df1.iloc[:,1] == df1.iloc[:,1].max()].index[0])*7+6,4])\n",
        "\n",
        "y1 = []\n",
        "y2 = []\n",
        "y3 = []\n",
        "y4 = []\n",
        "y5 = []\n",
        "\n",
        "x = np.arange(1, 8)\n",
        "\n",
        "for i in range(len(rbm4)):\n",
        "  y1.append(np.transpose(np.array([[rbm4[i], rbm8[i], rbm12[i], rbm16[i], rbm20[i], rbm24[i], rbm28[i]]])))\n",
        "  y2.append(np.transpose(np.array([[dae4[i], dae8[i], dae12[i], dae16[i], dae20[i], dae24[i], dae28[i]]])))\n",
        "  y3.append(np.transpose(np.array([[gbm4[i], gbm8[i], gbm12[i], gbm16[i], gbm20[i], gbm24[i], gbm28[i]]])))\n",
        "  y4.append(np.transpose(np.array([[kae4[i], kae8[i], kae12[i], kae16[i], kae20[i], kae24[i], kae28[i]]])))\n",
        "  y5.append(np.transpose(np.array([[pbm4[i], pbm8[i], pbm12[i], pbm16[i], pbm20[i], pbm24[i], pbm28[i]]])))\n",
        "\n",
        "\n",
        "mp1 = []\n",
        "for i in range(len(y1)):\n",
        "  mp1.append(np.mean(y1[i][:].astype(float)))\n",
        "\n",
        "mp2 = []\n",
        "for i in range(len(y2)):\n",
        "  mp2.append(np.mean(y2[i][:].astype(float)))\n",
        "\n",
        "mp3 = []\n",
        "for i in range(len(y3)):\n",
        "  mp3.append(np.mean(y3[i][:].astype(float)))\n",
        "\n",
        "mp4 = []\n",
        "for i in range(len(y4)):\n",
        "  mp4.append(np.mean(y4[i][:].astype(float)))\n",
        "\n",
        "mp5 = []\n",
        "for i in range(len(y5)):\n",
        "  mp5.append(np.mean(y5[i][:].astype(float)))\n",
        "\n",
        "max_value1 = min(mp1)\n",
        "index_max1 = mp1.index(max_value1)\n",
        "max_value2 = max(mp2)\n",
        "index_max2 = mp2.index(max_value2)\n",
        "max_value3 = max(mp3)\n",
        "index_max3 = mp3.index(max_value3)\n",
        "max_value4 = max(mp4)\n",
        "index_max4 = mp4.index(max_value4)\n",
        "max_value5 = max(mp5)\n",
        "index_max5 = mp5.index(max_value5)\n",
        "#__________________________\n",
        "\n",
        "bspl1 = []\n",
        "bspl2 = []\n",
        "bspl3 = []\n",
        "bspl4 = []\n",
        "bspl5 = []\n",
        "\n",
        "y6 = []\n",
        "y7 = []\n",
        "y8 = []\n",
        "y9 = []\n",
        "y10 = []\n",
        "\n",
        "for i in range(len(rbm4)):\n",
        "  y6.append(np.transpose(np.array([rbm4[i], rbm8[i], rbm12[i], rbm16[i], rbm20[i], rbm24[i], rbm28[i]])))\n",
        "  y7.append(np.transpose(np.array([dae4[i], dae8[i], dae12[i], dae16[i], dae20[i], dae24[i], dae28[i]])))\n",
        "  y8.append(np.transpose(np.array([gbm4[i], gbm8[i], gbm12[i], gbm16[i], gbm20[i], gbm24[i], gbm28[i]])))\n",
        "  y9.append(np.transpose(np.array([kae4[i], kae8[i], kae12[i], kae16[i], kae20[i], kae24[i], kae28[i]])))\n",
        "  y10.append(np.transpose(np.array([pbm4[i], pbm8[i], pbm12[i], pbm16[i], pbm20[i], pbm24[i], pbm28[i]])))\n",
        "\n",
        "for i in range(len(y6)):\n",
        "  bspl1.append(splrep(x, y6[i], k=3))\n",
        "  bspl2.append(splrep(x, y7[i], k=3))\n",
        "  bspl3.append(splrep(x, y8[i], k=3))\n",
        "  bspl4.append(splrep(x, y9[i], k=3))\n",
        "  bspl5.append(splrep(x, y10[i], k=3))\n",
        "\n",
        "x_new = np.linspace(1, 7, 100)\n",
        "\n",
        "#__________________________\n",
        "ax = plt.figure(figsize=(20, 7)).gca()\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "plt.title(\"Training Metrics\")\n",
        "plt.xlabel(\"cross-validation iterations\")\n",
        "plt.ylabel(\"metrics\")\n",
        "for i in range(len(y6)):\n",
        "  bspl_y1 = splev(x_new,bspl1[i])\n",
        "  bspl_y2 = splev(x_new,bspl2[i])\n",
        "  bspl_y3 = splev(x_new,bspl3[i])\n",
        "  bspl_y4 = splev(x_new,bspl4[i])\n",
        "  bspl_y5 = splev(x_new,bspl5[i])\n",
        "\n",
        "  if(i==index_max1):\n",
        "    plt.plot(x_new, bspl_y1.astype(float), color =\"grey\",linewidth=6.0)\n",
        "  if(i==index_max2):\n",
        "    plt.plot(x_new, bspl_y2.astype(float), color =\"green\",linewidth=6.0)\n",
        "  if(i==index_max3):\n",
        "    plt.plot(x_new, bspl_y3.astype(float), color =\"brown\",linewidth=6.0)\n",
        "  if(i==index_max4):\n",
        "    plt.plot(x_new, bspl_y4.astype(float), color =\"magenta\",linewidth=6.0)\n",
        "  if(i==index_max5):\n",
        "    plt.plot(x_new, bspl_y5.astype(float), color =\"pink\",linewidth=6.0)\n",
        "\n",
        "plt.legend([\"Loss\", \"Accuracy\", \"Precision\", \"F1\", \"Recall\"], loc =\"upper right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LOdlyfgF482U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}